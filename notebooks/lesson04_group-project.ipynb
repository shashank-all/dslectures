{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - Group Project\n",
    "\n",
    "> In this lesson we introduce the group project, its evaluation criteria and a submission example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lvwerra/dslectures/master?urlpath=lab/tree/notebooks%2Flesson04_group-project.ipynb) \n",
    "[![slides](https://img.shields.io/static/v1?label=slides&message=2021-lesson04.pdf&color=blue&logo=Google-drive)](https://drive.google.com/open?id=1SkS99lmu3zvr1vLe9ywDAIVMnnZ5FkqZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A guide to structuring your group projects\n",
    "\n",
    "_Summary:_ In your group projects you will solve a data science problem end-to-end, pretending to be recently hired data scientists in a company.  To help you get started, we've prepared a checklist to guide you through the project. Here are the main steps that you will go through:\n",
    "\n",
    "1. Frame the problem and look at the big picture\n",
    "2. Get the data\n",
    "3. Explore and visualise the data to gain insights\n",
    "4. Prepare the data to better expose the underlying data patterns to machine learning algorithms\n",
    "5. Explore many different models and short-list the best ones\n",
    "6. Fine-tune your models\n",
    "7. Present your solution\n",
    "\n",
    "In each step we list a set of questions that one should have in mind when undertaking a data science project. The list is not meant to be exhaustive, but does contain a selection of the most important questions to ask. We will be available to provide assitance with each of the steps, and will allocate some part of each lesson towards working on the projects.\n",
    "\n",
    "### Our expectations\n",
    "\n",
    "To streamline the grading, your group must submit a _**single**_ Jupyter notebook, structured in terms of the first 6 sections listed in this guide (the seventh will be presented to the class). You are welcome to adapt code from the web (e.g. Kaggle kernels), but you **_must_** reference the original source in your notebook.\n",
    "\n",
    "In addition to _clean, well-documented code_ (i.e. functions with docstrings etc), your notebook will be judged according to how well each step is explained (using Markdown). The main goal is to simulate what it is like to work as a data scientist, where communication is arguably as important as the ability to extract insights from data.\n",
    "\n",
    "The analysis in the Jupyter notebook will be evaluated according to a rubric similar to the assignments:\n",
    "\n",
    "| Critical Task | Needs Improvement | Basic | Surpassed |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Computation:** Perform computations | Computations contain errors and extraneous code | Computations are correct but contain extraneous/unnecessary computations | Computations are correct and properly identified and labeled |\n",
    "| **Analysis:** Choose and carry out analysis appropriate for data and context | Choice of analysis is overly simplistic, irrelevant, or missing key components | Analysis appropriate, but incomplete, or important features and assumptions not made explicit | Analysis appropriate, complete, advanced, relevant, and informative |\n",
    "| **Synthesis:** Identify key features of the analysis, and interpret results (including context) | Conclusions are missing, incorrect, or not made based on results of analysis | Conclusions reasonable, but is partially correct or partially complete | Make relevant conclusions explicitly connected to analysis and to context |\n",
    "| **Visual presentation:** Communicate findings graphically clearly, precisely, and concisely | Inappropriate choice of plots; poorly labeled plots; plots missing | Plots convey information correctly but lack context for interpretation | Plots convey information correctly with adequate/appropriate reference information |\n",
    "| **Written:** Communicate findings clearly, precisely, and concisely | Explanation is illogical, incorrect, or incoherent | Explanation is partially correct but incomplete or unconvincing | Explanation is correct, complete, and convincing |\n",
    "\n",
    "**Grading split:** The group project accounts for 50% of the final grading and is split equally between the notebook (25%) and the presentation (25%).\n",
    "\n",
    "**Submission deadline:** Wednesday, June 2, 2021 before 23:59:59 CEST (Notebook + presentation recording)\n",
    "\n",
    "**Presentation date:** Wednesday, June 9, 2021 (Discussion of group projects with questions)\n",
    "\n",
    "### Deliverables\n",
    "The teams have to submit two deliverables before the submission deadline: 1) a notebook and 2) presentation video.\n",
    "\n",
    "#### Notebook\n",
    "The notebook contains all the code to explore the dataset, train the final model and documents each step clearly. If code is copied from another codebase such as Github or Stack Overflow it **_must_** be properly referenced.\n",
    "\n",
    "#### Presentation\n",
    "The presentation video should be 15min long and should highlight the problem you are solving, interesting things you found in the data and the step involved in building up your model. On the presentation date we will discuss the presentation and ask questions about your project and submissions.\n",
    "\n",
    "### Some examples\n",
    "The Kaggle competitions [page](https://www.kaggle.com/competitions) has hundreds of examples where people have applied machine learning to solve a variety of problems. Below are a few examples that you might find useful:\n",
    "\n",
    "* Exploratory data analysis\n",
    "    * Regression: [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)\n",
    "* Model building\n",
    "    * Regression: [A study on Regression applied to the Ames dataset](https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset)\n",
    "    \n",
    "The is also the excellent [Kaggle Learn](https://www.kaggle.com/learn/overview) resource that you might find useful too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Frame the problem and look at the big picture\n",
    "1. Define the objective in business terms\n",
    "2. How should you frame the problem (supervised/unsupervised etc.)?\n",
    "3. How should performance be measured?\n",
    "4. How would you solve the problem manually?\n",
    "5. List the assumption you and your team have made so far\n",
    "6. Verify your assumptions if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the data\n",
    "1. Find and document where you can get the data from\n",
    "2. Create a workspace. We like to structure our project folders as follows:\n",
    "\n",
    "```\n",
    "my-awesome-project\n",
    "├── data\n",
    "│   ├── external       <- Data from third party sources.\n",
    "│   ├── interim        <- Intermediate data that has been transformed.\n",
    "│   ├── processed      <- The final, canonical data sets for modeling.\n",
    "│   └── raw            <- The original, immutable data dump.\n",
    "│\n",
    "├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n",
    "│                         the creator's initials, and a short \"-\" delimited description, e.g.\n",
    "│                         1.0-vwl-initial-data-exploration.\n",
    "│\n",
    "├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n",
    "│   └── figures        <- Generated graphics and figures to be used in reporting\n",
    "│\n",
    "└── requirements.txt   <- The requirements file for reproducing the analysis environment.\n",
    "```\n",
    "\n",
    "3. Once you and your team have agreed on the folder structure, we suggest creating a new virtual environment as follows in the root of `my-awesome-project`.\n",
    "4. Get the data\n",
    "5. Check the size and type of data (time series, geographical etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore the data\n",
    "1. Create a copy of the data for explorations (sampling it down to a manageable size if necessary)\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration\n",
    "3. Study each feature and its characteristics:\n",
    "    * Name\n",
    "    * Type (categorical, int/float, bounded/unbounded, text, structured, etc)\n",
    "    * Percentage of missing values\n",
    "    * Check for outliers, rounding errors etc\n",
    "4. For supervised learning tasks, identify the target(s)\n",
    "5. Visualise the data\n",
    "6. Study the correlations between features\n",
    "7. Study how you would solve the problem manually\n",
    "8. Identify the promising transformations you may want to apply (e.g. convert skewed targets to normal via a log transformation)\n",
    "10. Document what you have learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the data\n",
    "Notes:\n",
    "* Work on copies of the data (keep the original dataset intact).\n",
    "* Write functions for all data transformations you apply, for three reasons:\n",
    "    * So you can easily prepare the data the next time you run your code\n",
    "    * So you can apply these transformations in future projects\n",
    "    * To clean and prepare the test set\n",
    "    \n",
    "    \n",
    "1. Data cleaning:\n",
    "    * Fix or remove outliers (optional)\n",
    "    * Fill in missing values (e.g. with zero, mean, median, ...) or drop their rows (or columns)\n",
    "2. Feature selection (optional):\n",
    "    * Drop the features that provide no useful information for the task (e.g. a customer ID is usually useless for modelling).\n",
    "3. Feature engineering, where appropriate:\n",
    "    * Discretize continuous features\n",
    "    * Add promising transformations of features (e.g. $\\log(x)$, $\\sqrt{x}$, $x^2$, etc)\n",
    "    * Aggregate features into promising new features\n",
    "4. Feature scaling: standardise or normalise features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Short-list promising models\n",
    "We expect you to do some additional research and train at **least one model per team member**. You use a Random Forest model, but each team memeber has to investigate one additional model. So you may want to investigate the following alternatives for regression: \n",
    "- Linear Regression\n",
    "- Extra Trees\n",
    "- Histogram-based Gradient Boosting Regression Tree.\n",
    "- Multi-layer Perceptron regressor\n",
    "- Elastic-Net\n",
    "\n",
    "These additional models don't need to contribute to your final submission but there should for the individual models should be annotated with the student who created it. Each section should have the student name annotated. Each student should understand his model to the point where you can explain how it works and answer simple questions about it. \n",
    "\n",
    "1. Train mainy quick and dirty models from different categories (e.g. linear, SVM, Random Forests etc) using default parameters\n",
    "2. Measure and compare their performance\n",
    "3. Analyse the most significant variables for each algorithm\n",
    "4. Analyse the types of errors the models make\n",
    "5. Have a quick round of feature selection and engineering\n",
    "6. Have one or two more quick iterations of the five previous steps\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fine-tune the system\n",
    "1. Fine-tune the hyperparameters\n",
    "2. Once you are confident about your final model, measure its performance on the test set to estimate the generalisation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Present your solution\n",
    "1. Document what you have done\n",
    "2. Create a nice 15 minute video presentation with slides\n",
    "    * Make sure you highlight the big picture first\n",
    "3. Explain why your solution achieves the business objective\n",
    "4. Don't forget to present interesting points you noticed along the way:\n",
    "    * Describe what worked and what did not\n",
    "    * List your assumptions and you model's limitations\n",
    "5. Ensure your key findings are communicated through nice visualisations or easy-to-remember statements (e.g. \"the median income is the number-one predictor of housing prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* _Hands-On Machine Learning with Scikit-Learn and Tensorflow_, Appendix B, A. Géron \n",
    "* [Cookiecutter Data Science](http://drivendata.github.io/cookiecutter-data-science/#cookiecutter-data-science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset for the competition consists of a regression problem for music popularity. The data was scraped from the Spotify API and contains several features, such as the name of the song and how much accoustics is used in the song. In total there are 17 features in the dataset. The target for the competition is to predict the popularity of a song from these features. In total, the dataset contains 130'326 samples which are split into a train (9/10) and test (1/10) sets. The training data contains the feature a and label columns whereas the test set only contains the feature columns. The goal of the competition is to train a model on the training set and then use this model to predict the labels on the test set. A more detailed description of the data is available on the competition website.\n",
    "\n",
    "### Kaggle\n",
    "The competition is organized as a Kaggle challenge. The data is available on the Kaggle page and you have to upload you model predictions on the Kaggle page. Your results will automatically be evaluated and you will see your scores as well as the scores of the other teams on the leaderboard. Note that the test set is split into two parts: 50% is used to evaluate your predictions every time you upload them. The remaining 50% of the test set are not evaluated until the competition finishes and is used to calculate the final score. This split aims at avoiding teams improving overfitting their models to the test set score. The number of team submissions per day are limited to **20**. Therefore, make sure you distribute the work such that you can evaluate all your ideas during the competition.\n",
    "\n",
    "### Signup\n",
    "Go to the Kaggle website and click on `Register` to create an account. Once you have set up your account go the the competition page and click on `Teams`. Invite your fellow team member and name your team according the the MS Teams names.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "To upload your predictions store them as a csv file (see the `sample_submission.csv` for reference) and click on the `Submit Predictions` button on the competition page. Upload your submission in the dialog and add a short description of the steps that led these particular descriptions. After a few minutes you should see your score under  `My submissions` and if its your best run also on the `Leaderboard`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: The median regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('../data/spotify')\n",
    "train = pd.read_csv(datapath/'train.csv')\n",
    "test = pd.read_csv(datapath/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((117293, 18), (13033, 17))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>popularity</th>\n",
       "      <th>collection_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netherfriends</td>\n",
       "      <td>7luDJV4DDZmjH3QDdCqpcO</td>\n",
       "      <td>Money Everyday</td>\n",
       "      <td>0.0722</td>\n",
       "      <td>0.790</td>\n",
       "      <td>186410</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>-11.201</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>156.077</td>\n",
       "      <td>4</td>\n",
       "      <td>0.203</td>\n",
       "      <td>2</td>\n",
       "      <td>april-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maxo Kream</td>\n",
       "      <td>1F09jtMfeYNkUj6piQjXwM</td>\n",
       "      <td>ATW</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.677</td>\n",
       "      <td>149967</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>-5.680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3180</td>\n",
       "      <td>153.921</td>\n",
       "      <td>4</td>\n",
       "      <td>0.371</td>\n",
       "      <td>41</td>\n",
       "      <td>april-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drebae</td>\n",
       "      <td>2tR2VS8rAndKb0C5hS3IpD</td>\n",
       "      <td>Trust in Me (feat. Allura)</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.661</td>\n",
       "      <td>231732</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0971</td>\n",
       "      <td>-8.090</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>132.899</td>\n",
       "      <td>4</td>\n",
       "      <td>0.385</td>\n",
       "      <td>9</td>\n",
       "      <td>april-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TWICE</td>\n",
       "      <td>1U3cHXWaa0FqlUWLLBL7Kz</td>\n",
       "      <td>BRAND NEW GIRL</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.674</td>\n",
       "      <td>213956</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>-3.754</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>157.041</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750</td>\n",
       "      <td>41</td>\n",
       "      <td>april-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lisa Howard</td>\n",
       "      <td>1vAP99gg1HzuUUITxJPdyn</td>\n",
       "      <td>Cheeseburger In Paradise</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>0.607</td>\n",
       "      <td>228556</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>-5.106</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>138.087</td>\n",
       "      <td>4</td>\n",
       "      <td>0.496</td>\n",
       "      <td>19</td>\n",
       "      <td>april-2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     artist_name                track_id                  track_name  \\\n",
       "0  Netherfriends  7luDJV4DDZmjH3QDdCqpcO              Money Everyday   \n",
       "1     Maxo Kream  1F09jtMfeYNkUj6piQjXwM                         ATW   \n",
       "2         Drebae  2tR2VS8rAndKb0C5hS3IpD  Trust in Me (feat. Allura)   \n",
       "3          TWICE  1U3cHXWaa0FqlUWLLBL7Kz              BRAND NEW GIRL   \n",
       "4    Lisa Howard  1vAP99gg1HzuUUITxJPdyn    Cheeseburger In Paradise   \n",
       "\n",
       "   acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
       "0        0.0722         0.790       186410   0.384          0.000002    7   \n",
       "1        0.1550         0.677       149967   0.747          0.000000    6   \n",
       "2        0.2390         0.661       231732   0.625          0.000000    6   \n",
       "3        0.0160         0.674       213956   0.965          0.000096    7   \n",
       "4        0.2810         0.607       228556   0.771          0.000005    2   \n",
       "\n",
       "   liveness  loudness  mode  speechiness    tempo  time_signature  valence  \\\n",
       "0    0.1000   -11.201     1       0.2310  156.077               4    0.203   \n",
       "1    0.2390    -5.680     0       0.3180  153.921               4    0.371   \n",
       "2    0.0971    -8.090     0       0.1010  132.899               4    0.385   \n",
       "3    0.1450    -3.754     1       0.0696  157.041               4    0.750   \n",
       "4    0.2210    -5.106     1       0.1120  138.087               4    0.496   \n",
       "\n",
       "   popularity collection_date  \n",
       "0           2      april-2019  \n",
       "1          41      april-2019  \n",
       "2           9      april-2019  \n",
       "3          41      april-2019  \n",
       "4          19      april-2019  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "In this example we build a simple model that just consists of the training set median of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = train['popularity'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on the test set\n",
    "Normally you would then use the trained model to predict the target on the test set with `.predict(X)`. Our simple model does not depend on the features so we can just assign the median value to all test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test[['track_id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7rrY55kdGxRC65rfeeJkG0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bLPm7gEeoNtn6R0YNAcBS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4pRyBsQtXo9nCkPjFQJmZT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6zngmeIx1bCRvgLM53NJ2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01XKJqKTothbGn7VOzQcdb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id\n",
       "0  7rrY55kdGxRC65rfeeJkG0\n",
       "1  0bLPm7gEeoNtn6R0YNAcBS\n",
       "2  4pRyBsQtXo9nCkPjFQJmZT\n",
       "3  6zngmeIx1bCRvgLM53NJ2e\n",
       "4  01XKJqKTothbGn7VOzQcdb"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['popularity'] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7rrY55kdGxRC65rfeeJkG0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bLPm7gEeoNtn6R0YNAcBS</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4pRyBsQtXo9nCkPjFQJmZT</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6zngmeIx1bCRvgLM53NJ2e</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01XKJqKTothbGn7VOzQcdb</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id  popularity\n",
       "0  7rrY55kdGxRC65rfeeJkG0        21.0\n",
       "1  0bLPm7gEeoNtn6R0YNAcBS        21.0\n",
       "2  4pRyBsQtXo9nCkPjFQJmZT        21.0\n",
       "3  6zngmeIx1bCRvgLM53NJ2e        21.0\n",
       "4  01XKJqKTothbGn7VOzQcdb        21.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save submission\n",
    "Finally, we take the predictions on the test set and save them in a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(datapath/'median_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now upload this file on the Kaggle competition webpage under `Submit Prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
